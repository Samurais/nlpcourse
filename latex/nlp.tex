Introduction

Natural language processing is a large field in the intersection of
linguistics (including phonology, morphology, syntax, semantics,
discourse, pragmatics), and computation (computability, complexity,
learnability).  This course will focus on aspects of NLP related to
extracting useful information from text (we will not talk about how we
get the text, e.g. speech recognition; or how we produce text,
e.g. natural language generation and summarization).  In particular we
will concentrate on methods that {\em learn} how to extract useful
information from text from given examples.  From a machine learning
perspective we imagine there is a function 

\[ f: X -> Y \]

which maps the input ($x \in X$ which in our case will be text), to an
output in a specified set $y \in Y$ and we would like to learn what
this function is by observing a number of $(x, y)$ examples ((this is
only supervised learning from annotated datasets)).  Typically the
mapping from $X$ to $Y$ may not be fully determined due to ambiguity,
missing information, etc.  In that case we will use a scoring function
as a target of our learning ((do we ever use f?))

\[ score: X x Y -> R \]

where $score(x, y)$ determines how good the output $y$ is for the
input $x$.  The score may or may not have a probabilistic
interpretation, and when it does we may choose to represent the joint
probability $p(x, y)$ or the conditional probability $p(y|x)$.
Working with probabilities have certain advantages ((what exactly)),
but require normalization (probabilities have to add to 1) which may
increase our computational cost ((why exactly are we paying this
price, what is wrong with unnormalized probabilities?)).  ((isn't all
this a bit too much detail before we give any examples of x and y?))
((how about unsupervised?))

The information extracted from text could be useful in and of itself
(sentiment analysis, spam detection), or part of a longer chain of
intermediate representations that we hope will lead to semantic
analysis (tagging, parsing).  We will categorize NLP problems based on
the complexity of the output:

\begin{itemize}
\item Text -> Label: spam detection, sentiment analysis, topic
  detection.
\item Text -> Probability: language modeling, with applications to
  lexical substitution, disambiguation.
\item Text (word sequence) -> Label sequence: tagging and chunking:
  POS, WSD, NER, NP chunking, morphological disambiguation.
\item Text (word sequence) -> Another word sequence: statistical
  machine translation.  The difference between this and labeling is
  (1) labeling is one-to-one, (2) monotonically ordered, (3) small
  vocabulary, none of which holds for SMT.
\item Text -> Trees: parsing, dependency, constituent, ccg.
\item Text -> Semantics: ((need to research this))
\end{itemize}

In machine learning terminology mapping inputs to discrete labels is
called classification, mapping inputs to numbers is called regression
and learning to estimate probabilities of a set of objects is called
density estimation.  Anything beyond that (starting with label
sequences) where the output has certain structure and components of
the output start to constrain and correlate with each other, is called
structured prediction.  ((give examples of e.g. postag interaction))

As the complexity of the output increases, so does the complexity of
learning and decoding (deciding on the best output).  Thus one way to
push the field forward is to investigate ways to learn and decode more
complex output sets.  Another way forward is to reduce the amount of
supervision and demonstrate learning where part or all of the examples
is missing their output (semisupervised and unsupervised learning), or
part of the input is missing (hidden variable learning).  ((How about
the example of using logic formulas vs just the answers in GeoQuery?
Knowing words vs trying to learn them from formulas?))


Datasets, References, and Examples

Many datasets for below tasks exist in nltk:
http://nltk.googlecode.com/svn/trunk/nltk_data/index.xml

1. CLASSIFICATION

Can we introduce generative, conditional, large margin, discriminative
etc. using this simple setting?  We can't introduce decoding though!

1.1 SPAM DETECTION
Ion Androutsopoulos got 500+ citations in a couple of papers:
http://www.aclweb.org/aclwiki/index.php?title=Spam_filtering_datasets
http://csmining.org/index.php/enron-spam-datasets.html
http://csmining.org/index.php/ling-spam-datasets.html
http://csmining.org/index.php/pu1-and-pu123a-datasets.html
http://csmining.org/index.php/spam-assassin-datasets.html
http://archive.ics.uci.edu/ml/datasets/Spambase: not good, already feature extracted

1.2 SENTIMENT ANALYSIS
The following paper introduced movie review data and got 2612 citations:
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. Thumbs up? Sentiment classiﬁcation using
machine learning techniques. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 79–86, 2002.
- nltk uses this dataset for text classification example.
http://www.cs.cornell.edu/home/llee/opinion-mining-sentiment-analysis-survey.html
http://www.cs.cornell.edu/people/pabo/movie-review-data/
Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews. PD Turney - Proceedings of the 40th annual meeting on association …, 2002 - dl.acm.org: Earlier paper, cited by Pang and Lee.


1.3 TEXT CLASSIFICATION
ir-book (2009) chap 13 is on classification: gives reuters, modapte split of reuters-21578 is standard, good review
manning chap 16 is on TC, uses reuters, explores different classification algorithms, introduces idtree, maxent, nb, perceptron, knn, does not mention svm, logistic regression
Top cited paper: McCallum1998 uses 5 datasets: Yahoo Science, Industry Sector, 20-newsgroups, webKB, Reuters-21578.
http://csmining.org/index.php/id-20-newsgroups.html
http://csmining.org/index.php/r52-and-r8-of-reuters-21578.html
http://qwone.com/~jason/20Newsgroups/
http://www.cs.cmu.edu/~TextLearning/datasets.html: has webkb, 20-newsgroups, industry-sector
/ai/data/nlp/reuters has both the old and new (rcv1) versions of reuters
http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/lyrl2004_rcv1v2_README.htm -- standard ref for rcv1
jurafsky has nothing
This should lead to Blei2003 (LDA, cited 6708) and Teh2005 (HDP, cited 1362).  LDA uses TREC_AP corpus, scientific abstracts from C-elegans, compares with LSI, uses Reuters-21578 for dim reduction compares to all-words using Joachims1999 svmlight.  HDP uses C-elegans, NIPS articles with shared topics.  Compares infinite HMM with Stolcke-Omohundro bayesian model merging.

1.4 WORD CLUSTERING
isn't this close to unsupervised text classification?
Brown clusters use HMM.
How about LSA?  Can we go supervised text classification -> text clustering -> word clustering?
manning chap 14 is on clustering, 15.4 is on LSI, what is LSI vs LSA?
manning chap 15 is on IR, introduces vector space models, LSI, mentions Reuters (is introduced by Lewis 1992?)
DekangLin1998 also has stuff.

2. SLM
We can use the upos data (wsj)?
Can we do Turkish?  (would need morphologically split data, handling of unknowns etc.)
SRILM: [PDF] SRILM-an extensible language modeling toolkit. A Stolcke - INTERSPEECH, 2002 - (2502 citations)
An empirical study of smoothing techniques for language modeling. SF Chen, J Goodman - Proceedings of the 34th annual meeting on …, 1996 - cited by 1411, but the following is more informative:
A bit of progress in language modeling JT Goodman - Computer Speech & Language, 2001 - Elsevier (395 citations)
He used NAB:
All of our experiments were performed on the NAB (North American Business news) corpus (Stern, 1996). We performed most experiments at 4 diﬀerent
training data sizes: 100,000 words, 1,000,000 words, 10,000,000 words, and the
whole corpus – except 1994 WSJ data – approximately 284,000,000 words. In
all cases, we performed parameter optimization on a separate set of heldout
data, and then performed testing on a set of test data. None of the three data
sets overlapped. The heldout and test sets were always every ﬁftieth sentence
from two non-overlapping sets of 1,000,000 words, taken from the 1994 section.
  We also have:
Two decades of statistical language modeling: Where do we go from here?  R Rosenfeld - Proceedings of the IEEE, 2000 - ieeexplore.ieee.org
An empirical study of smoothing techniques for language modeling. SF Chen, J Goodman - Computer Speech & Language, 1999 - Elsevier
A hierarchical Bayesian language model based on Pitman-Yor processes.  YW Teh - Proceedings of the 21st International Conference on …, 2006 - dl.acm.org
Improved backing-off for m-gram language modeling.  R Kneser, H Ney - … , and Signal Processing, 1995. ICASSP-95., …, 1995 - ieeexplore.ieee.org

3. TAGGING AND CHUNKING

3.1 NP CHUNKING 
- conll 1999-2000

3.2 NER
- conll 2002-2003

3.3 POS
- PTB WSJ
Feature-rich part-of-speech tagging with a cyclic dependency network. K Toutanova, D Klein, CD Manning… - Proceedings of the 2003 …, 2003 - dl.acm.org
[PDF] A maximum entropy model for part-of-speech tagging.  A Ratnaparkhi - Proceedings of the conference on empirical …, 1996 - acl.ldc.upenn.edu

3.4 MOR
- My dataset?
Two-level description of Turkish morphology.  K Oflazer - Literary and linguistic computing, 1994 - ALLC
Dependency parsing of Turkish. G Eryiğit, J Nivre, K Oflazer - Computational Linguistics, 2008 - MIT Press
Finite State Morphology textbook.
Xerox tools and its free versions?
Need to talk about FST if we are going to learn it:
Stolcke and Omohundro, Xavier Carreras, HDP paper etc.
Morphochallenge?
Disambiguation just tagging but learning anything else goes deeper.

3.5 WSD
- Semeval?

4. SMT
Sentence alignment: Gale and Church 1993
Word alignment: Brown 1990
Check out Noah 1.9, 1.10.

5. PARSING

5.1 PS-PARSING
- PTB WSJ, standard split
Building a large annotated corpus of English: The Penn Treebank.  MP Marcus, MA Marcinkiewicz, B Santorini - Computational linguistics, 1993 - dl.acm.org
Study Charniak, Collins, Stanford, and Berkeley models?
A maximum-entropy-inspired parser. E Charniak - Proceedings of the 1st North American chapter of the …, 2000 - dl.acm.org
Head-driven statistical models for natural language parsing.  M Collins - Computational linguistics, 2003 - MIT Press
Accurate unlexicalized parsing.  D Klein, CD Manning - Proceedings of the 41st Annual Meeting on …, 2003 - dl.acm.org
Petrov and Klein: which reference?  Noah says 2008.
de Marnffe and Manning: Stanford typed dependencies
Unsupervised?  Rens Bod?  DMV, CCM?
TAG?  TSG?

5.2 DEP-PARSING
- conll 2006-2007
Dependency parsing of Turkish. G Eryiğit, J Nivre, K Oflazer - Computational Linguistics, 2008 - MIT Press
MaltParser: A language-independent system for data-driven dependency parsing.  J Nivre, J Hall, J Nilsson, A Chanev… - Natural Language …, 2007 - Cambridge Univ Press
Non-projective dependency parsing using spanning tree algorithms. R McDonald, F Pereira, K Ribarov, J Hajič

5.3 CCG-PARSING
Clark and Curran 2004: CCG parser
Carreras, Collins, Koo 2008: TAG parsing
Riezler 2000: constraint based?

6. SEMANTIC PARSING
- GeoQuery: Zelle and Mooney 1996, Wong and Mooney 2007, Zettlemoyer and Collins 2005, Clarke and Goldwasser 2010, Liang-Jordan-Klein-2011/2013
- Robot: Ge and Mooney 2005
- Travel: Mooney 1997
- Vision

0. SRL
which math?
conll 2004, 2005
Automatic labeling of semantic roles.  D Gildea, D Jurafsky - Computational linguistics, 2002 - MIT Press
The proposition bank: An annotated corpus of semantic roles. M Palmer, D Gildea, P Kingsbury - Computational Linguistics, 2005 - MIT Press


0. COREF
which math?
conll 2011, 2012
Ng and Cardie 2002
Culotta, McCallum 2007
Haghighi, Klein 2009
Poon, Domingos 2008

Introduce WordNet (Fellbaum1998) and Celex?
ConceptNet, PropBank (Palmer2005), FrameNet (Baker1998), NomBank, Ontonotes?
Discourse treebank (Carlson2003), Discourse graphbank (Wolf2005), Penn Discourse treebank (Prasad2008), Discourse tutorial (Webber2010)
Concentrate on learning not problems and resources!

From Noah review by Quirk: It is a comprehensive introduction to the most common and effective decoding
approaches, with one signiﬁcant exception: the recent advances in dual decomposition
and Lagrangian relaxation methods. Timing is likely the culprit. This book was developed mainly from 2006 to 2009, whereas dual decomposition did not attain notoriety in
our community until a few years later (Rush et al. 2010). Relaxation approaches, though
potentially a passing phase, have successfully broadened the reach of simpler decoding
techniques into more complicated domains such as structured event extraction.

Hard EM is mentioned in the context
of several examples, though a more detailed description of this potentially important
technique (cf. Spitkovsky et al., 2010) would bridge the material of Chapters 2 and 4.
